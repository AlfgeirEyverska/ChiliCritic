{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import itertools\n",
    "import csv\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChiliCritic\n",
    "#### By: Tyler Korte | 09/10/2020\n",
    "\n",
    "### Synopsis\n",
    "My employer recently hosted a chili cook-off and one of the more favored chilis ran out before the last few people could rate it. I was on awaiting access for my security clearance so I set out to use machine learning to predict how they would have voted on that chili based on how they rated the other chilis. The idea is that the machine learning will detect the patterns in preference between the voters that tried the last chili and those that didn't. For the official competition I suggested giving the final chili the average score of all previous votes and I will compare the results.\n",
    "\n",
    "Each chili was to be given a score in the range \\[1, 5\\] and some people used decimal answers (one used pi).\n",
    "\n",
    "For this project, I chose to use <a href=\"https://pytorch.org/\">pytorch</a>.\n",
    "\n",
    "### Data Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    inputs = list()\n",
    "    with open(filename, 'r') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "        for r in csv_reader:\n",
    "            inputs.append(r)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and test data came in the form of .csv files where the votes are positional and the final position represents the vote for the chili that ran out. The input data is also a .csv file, but it is missing the final data point.\n",
    "\n",
    "I chose to use the first 7 numbers as input and the last as the target output.\n",
    "\n",
    "### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(7, 12)\n",
    "        self.fc2 = nn.Linear(12, 12)\n",
    "        self.fc3 = nn.Linear(12, 1)\n",
    "        # self.fc4 = nn.Linear(12, 1)\n",
    "        # self.fc5 = nn.Linear(4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.fc5(torch.tanh(self.fc4(torch.tanh(self.fc3(torch.tanh(self.fc2(torch.tanh(self.fc1(x)))))))))\n",
    "        # x = self.fc3(F.relu(self.fc2(F.relu(self.fc1(x)))))\n",
    "        x = self.fc3(torch.sigmoid(self.fc2(torch.sigmoid(self.fc1(x)))))\n",
    "        # x = self.fc4(torch.sigmoid(self.fc3(torch.sigmoid(self.fc2(torch.sigmoid(self.fc1(x)))))))\n",
    "        return x\n",
    "    \n",
    "# Setting up the neural net\n",
    "net = Net()\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)  # , momentum=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I define the model as a fully-connected, linear model with a sigmoid activation function. This model has an input, hidden, and output layer. I have left in some of the other things I tried. Extra layers and different activation functions did not make a justifiable difference. This network uses the L1Loss function which calculates the mean absolute error (MAE) to do stochastic gradient descent.\n",
    "\n",
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = load_data('input_data.csv')\n",
    "raw_validation_data = load_data('validation_data.csv')\n",
    "raw_training_data = load_data('training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = list()\n",
    "for row in raw_validation_data:\n",
    "    validation_data.append((row[0:-1], row[-1]))\n",
    "\n",
    "training_data = list()\n",
    "for row in raw_training_data:\n",
    "    training_data.append((row[0:-1], row[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Training Data                  -                 Test Data                   ]\n",
      "([5.0, 3.0, 3.0, 3.0, 3.0, 3.0, 5.0], 4.0)   -   ([5.0, 3.0, 5.0, 2.0, 3.0, 3.0, 5.0], 4.0)]\n",
      "([5.0, 2.0, 3.0, 2.0, 3.0, 3.0, 4.0], 2.0)   -   ([1.0, 2.0, 3.0, 3.0, 4.0, 1.0, 5.0], 5.0)]\n",
      "([5.0, 3.0, 4.0, 3.0, 3.0, 2.0, 3.0], 5.0)   -   ([4.0, 5.0, 4.0, 3.0, 5.0, 4.0, 4.0], 3.0)]\n",
      "([4.0, 3.0, 3.0, 2.0, 2.0, 2.0, 4.0], 3.0)   -                 Input Data                  ]\n",
      "([5.0, 5.0, 5.0, 3.0, 3.0, 3.0, 4.0], 4.0)   -   [4.0, 3.0, 1.0, 2.0, 2.0, 1.0, 4.0]]\n",
      "([3.0, 3.0, 3.0, 3.0, 3.0, 5.0, 3.0], 3.0)   -   [5.0, 3.0, 5.0, 3.0, 3.0, 3.0, 3.0]]\n",
      "([3.0, 1.0, 5.0, 4.0, 3.0, 4.0, 5.0], 4.0)   -   [4.0, 1.0, 5.0, 3.0, 2.0, 1.0, 3.0]]\n",
      "([5.0, 1.0, 4.0, 3.0, 5.0, 2.0, 3.0], 4.0)   -   [3.0, 2.0, 3.0, 2.0, 2.0, 4.0, 2.0]]\n",
      "([3.0, 2.0, 4.0, 3.0, 4.0, 3.0, 2.0], 3.0)   -   [3.0, 2.0, 3.0, 2.0, 4.0, 3.0, 3.0]]\n",
      "([5.0, 4.0, 3.0, 2.0, 3.0, 2.0, 4.0], 5.0)   -                                    ]\n",
      "([4.0, 3.0, 2.0, 3.0, 1.0, 3.0, 3.0], 5.0)   -                                    ]\n",
      "([3.0, 4.0, 3.0, 2.0, 3.0, 1.0, 4.0], 5.0)   -                                    ]\n",
      "([5.0, 4.0, 4.0, 3.0, 3.0, 4.0, 5.0], 3.0)   -                                    ]\n",
      "([2.0, 5.0, 3.0, 1.0, 2.0, 2.0, 3.0], 4.0)   -                                    ]\n",
      "([5.0, 3.0, 4.0, 3.0, 4.0, 3.0, 3.0], 4.0)   -                                    ]\n",
      "([3.0, 2.0, 4.0, 1.0, 3.0, 2.0, 4.0], 4.0)   -                                    ]\n",
      "([3.0, 2.0, 4.0, 3.0, 3.0, 5.0, 5.0], 4.0)   -                                    ]\n",
      "([3.0, 3.0, 4.0, 3.5, 3.5, 2.0, 4.0], 3.0)   -                                    ]\n",
      "([2.0, 1.0, 2.0, 2.0, 2.0, 4.0, 3.0], 2.0)   -                                    ]\n",
      "([3.0, 3.0, 5.0, 2.0, 3.0, 3.0, 3.0], 3.0)   -                                    ]\n",
      "([5.0, 1.0, 2.0, 5.0, 1.0, 4.0, 5.0], 3.0)   -                                    ]\n",
      "([2.0, 3.0, 1.0, 2.5, 3.14285714285714, 1.0, 2.0], 5.0)   -                                    ]\n",
      "([2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0], 4.0)   -                                    ]\n",
      "([4.0, 4.0, 4.0, 2.0, 3.0, 3.0, 2.0], 3.0)   -                                    ]\n",
      "([3.0, 2.0, 5.0, 3.0, 4.0, 1.0, 4.0], 5.0)   -                                    ]\n",
      "([5.0, 3.0, 3.0, 2.0, 4.0, 2.0, 3.0], 4.0)   -                                    ]\n",
      "([5.0, 2.0, 4.0, 1.0, 2.0, 3.0, 4.0], 4.0)   -                                    ]\n",
      "([4.0, 2.0, 3.0, 3.0, 3.0, 3.0, 2.0], 5.0)   -                                    ]\n",
      "([5.0, 1.0, 3.0, 1.0, 3.0, 2.0, 2.0], 5.0)   -                                    ]\n",
      "([4.0, 2.0, 4.0, 3.0, 5.0, 2.0, 5.0], 4.0)   -                                    ]\n",
      "([5.0, 2.0, 3.0, 4.0, 3.0, 3.0, 3.0], 5.0)   -                                    ]\n",
      "([5.0, 3.0, 4.0, 4.0, 4.0, 4.0, 4.0], 5.0)   -                                    ]\n",
      "([4.0, 2.0, 4.0, 4.0, 5.0, 2.0, 3.0], 5.0)   -                                    ]\n",
      "([5.0, 3.0, 2.0, 3.0, 3.0, 4.0, 3.0], 3.0)   -                                    ]\n",
      "([4.0, 3.0, 5.0, 3.0, 4.0, 4.0, 4.0], 5.0)   -                                    ]\n",
      "([4.0, 2.0, 2.0, 3.0, 3.0, 4.0, 3.0], 3.0)   -                                    ]\n",
      "([1.0, 4.0, 3.0, 4.0, 5.0, 1.0, 5.0], 5.0)   -                                    ]\n",
      "([5.0, 3.0, 3.0, 3.0, 3.0, 2.0, 3.0], 4.0)   -                                    ]\n",
      "([5.0, 2.0, 4.0, 1.0, 5.0, 1.0, 2.0], 3.0)   -                                    ]\n"
     ]
    }
   ],
   "source": [
    "training_format_data = ['              Training Data               '] + training_data\n",
    "format_data = ['              Test Data                   '] + validation_data + ['              Input Data                  '] + input_data\n",
    "for x, y in itertools.zip_longest(training_format_data, format_data, fillvalue=' '*33):\n",
    "    print(f\"{x}   -   {y}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am separating the last piece of the data from the rest as the input and output.\n",
    "\n",
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naming scheme\n",
    "# ChiliCritic_LossFN_ActivationFN_HiddenLayers_Size\n",
    "nn_path = 'NeuralNets/ChiliCritic_L1_Sigmoid_1_12_725'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I check to see if a trained model exists to save time retraining it. I used the naming convention listed in order to retain different trained models for comparison while I was tweaking hyperperameters. If no saved model exists with the specified name, I train and save the model under that name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 0 --- Loss: 1.2479742765426636\n",
      "prediction: 1.7520257234573364  actual: 3.0\n",
      "    Epoch 76 --- Loss: 1.346364974975586\n",
      "prediction: 4.346364974975586  actual: 3.0\n",
      "    Epoch 152 --- Loss: 1.3724098205566406\n",
      "prediction: 4.372409820556641  actual: 3.0\n",
      "    Epoch 228 --- Loss: 1.239067554473877\n",
      "prediction: 4.239067554473877  actual: 3.0\n",
      "    Epoch 304 --- Loss: 1.2394137382507324\n",
      "prediction: 4.239413738250732  actual: 3.0\n",
      "    Epoch 380 --- Loss: 0.9669528007507324\n",
      "prediction: 3.9669528007507324  actual: 3.0\n",
      "    Epoch 456 --- Loss: 1.0361099243164062\n",
      "prediction: 4.036109924316406  actual: 3.0\n",
      "    Epoch 532 --- Loss: 0.9819433689117432\n",
      "prediction: 3.981943368911743  actual: 3.0\n",
      "    Epoch 608 --- Loss: 0.9252259731292725\n",
      "prediction: 3.9252259731292725  actual: 3.0\n",
      "    Epoch 684 --- Loss: 0.8569152355194092\n",
      "prediction: 3.856915235519409  actual: 3.0\n"
     ]
    }
   ],
   "source": [
    "if path.exists(nn_path):\n",
    "    net = torch.load(nn_path)\n",
    "    net.eval()\n",
    "else:\n",
    "\n",
    "    # Training the neural net\n",
    "    for epoch in range(725):\n",
    "        # i is a counter, dat represents the row in the data\n",
    "        for i, dat in enumerate(training_data):\n",
    "            # X represents the input data, Y represents the actual output\n",
    "            X, Y = iter(dat)\n",
    "            X, Y = Variable(torch.FloatTensor(X), requires_grad=True), Variable(torch.FloatTensor([Y]), requires_grad=False)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(X)\n",
    "\n",
    "            loss = loss_fn(outputs, Y)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # if i % 5 == 0:\n",
    "        if epoch % 76 == 0:\n",
    "            print(f'    Epoch {epoch} --- Loss: {loss.data.item()}')\n",
    "            print(f'prediction: {outputs[0]}  actual: {Y[0]}')\n",
    "    # Save NN so it doesn't need to be recomputed\n",
    "    torch.save(net, nn_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChiliCritic_L1_Sigmoid_1_12_725\n",
      "prediction: 4.065948486328125  actual: 4.0  difference: -0.1  percent difference: -2.5%\n",
      "prediction: 4.59492301940918  actual: 5.0  difference: 0.4  percent difference: 8.0%\n",
      "prediction: 2.9165267944335938  actual: 3.0  difference: 0.1  percent difference: 3.3%\n"
     ]
    }
   ],
   "source": [
    "print(nn_path.split('/')[-1])\n",
    "# Testing the neural net\n",
    "for i, dat in enumerate(validation_data):\n",
    "    # X represents the input data, Y represents the actual output\n",
    "    X, Y = iter(dat)\n",
    "    X, Y = Variable(torch.FloatTensor(X), requires_grad=True), Variable(torch.FloatTensor([Y]), requires_grad=False)\n",
    "\n",
    "    outputs = net(X)\n",
    "\n",
    "    loss = loss_fn(outputs, Y)\n",
    "\n",
    "    prediction = outputs[0]\n",
    "    actual = Y[0]\n",
    "    difference = round(float(Y[0] - outputs[0]), 1)\n",
    "    pd = round(float((difference/actual)*100), 1)\n",
    "    print(f'prediction: {prediction}  actual: {actual}  difference: {difference}  percent difference: {pd}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Model to Predict Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: 5\n",
      "prediction: 5\n",
      "prediction: 6\n",
      "prediction: 3\n",
      "prediction: 3\n",
      "4.4\n"
     ]
    }
   ],
   "source": [
    "# Guesses for the unknowns\n",
    "guesses = list()\n",
    "for i, dat in enumerate(input_data):\n",
    "    # X represents the input data, Y represents the actual output\n",
    "    X = dat\n",
    "    X = Variable(torch.FloatTensor(X), requires_grad=False)\n",
    "\n",
    "    outputs = net(X)\n",
    "\n",
    "    prediction = round(float(outputs[0]))\n",
    "    guesses.append(prediction)\n",
    "    print(f'prediction: {prediction}')\n",
    "\n",
    "print(sum(guesses)/len(guesses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Takeaways\n",
    "I believe that I was able to get reasonable results given the sample size. There is an issue where the third prediction is out of the range of possible values at 6 (that person would have loved the chili). \n",
    "\n",
    "The average of the scores for the chili before it ran out was 3.95238095238095 and this model predicts near that. The average is certainly adequate for declaring a winner, but it does not account for personal preference at all. This model predicts the same outcome and this chili (which everyone liked so much that it ran out) still won. I did not look at the data to see if it was possible for 5 votes to dethrone this winner, but I recall it being close.\n",
    "\n",
    "Looking back, I definitely overfit the data with 725 epochs and only 39 pieces of training data. I probably should have divided all of the data by 5 before use and multiplied by 5 and rounded in the end to prevent this issue. Maybe one day I will revisit this project and maybe even use tensorflow as a comparison.\n",
    "\n",
    "Author: <a href=\"https://tylerkorte.com\">Tyler Korte</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
